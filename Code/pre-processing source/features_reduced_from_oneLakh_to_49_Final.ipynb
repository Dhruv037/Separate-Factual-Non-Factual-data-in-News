{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import corenlp\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize, wordpunct_tokenize, word_tokenize, RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, math, glob, re\n",
    "\n",
    "from collections import Counter\n",
    "import re as regex\n",
    "import contractions\n",
    "import copy\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 9999999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "def regTokenize(text):\n",
    "    tok = RegexpTokenizer('[A-Za-z0-9]*[.]?\\w+')\n",
    "    return tok.tokenize(text)\n",
    "\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def lemma(words):\n",
    "    for i in range(0, len(words)):\n",
    "        words[i] = WordNetLemmatizer().lemmatize(words[i])\n",
    "    return words\n",
    "\n",
    "\n",
    "def stemming(words):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    for i in range(0, len(words)):\n",
    "        words[i] = porter_stemmer.stem(words[i])\n",
    "    return words\n",
    "\n",
    "\n",
    "def comma(text):\n",
    "    text = \"\".join(c for c in text if c not in ('!', '.', ':', ',', '\"', '?', '(', ')'))\n",
    "    return text\n",
    "\n",
    "\n",
    "def getBasicNorm(text):\n",
    "    for i in range(0, len(text)):\n",
    "        text[i] = contract(text[i])\n",
    "        text[i] = lowercase(text[i])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWholeString(data):\n",
    "    string = ''\n",
    "    for i in data:\n",
    "        string += i[0] + ' '\n",
    "    return string\n",
    "\n",
    "\n",
    "def getNGrams(corpusSentence):\n",
    "    corpusSentence = contract(corpusSentence)\n",
    "    corpusSentence = lowercase(corpusSentence)\n",
    "    tokenized = regTokenize(corpusSentence)\n",
    "    uni = list(ngrams(tokenized, 1))\n",
    "    bigram = list(ngrams(tokenized, 2))\n",
    "    trigram = list(ngrams(tokenized, 3))\n",
    "    print('uni:', len(uni), ' bi:', len(bigram), 'tri:', len(trigram))\n",
    "    grams = [uni, bigram, trigram]\n",
    "    return grams\n",
    "\n",
    "\n",
    "def postagging(corpusSentence):\n",
    "    corpusSentence = contract(corpusSentence)\n",
    "    corpusSentence = lowercase(corpusSentence)\n",
    "    doc = nlp(corpusSentence)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def posPattern(corpusSentence):\n",
    "    doc = nlp(corpusSentence)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# -----------------------------------------POS Pattern-----------------------------------------\n",
    "def getPosPattern(data_sentences):\n",
    "    sentences_pos = []\n",
    "    for i in data_sentences:\n",
    "        sent = []\n",
    "        s = postagging(i)\n",
    "        for j in s:\n",
    "            sent.append(j.pos_)\n",
    "        sentences_pos.append(sent)\n",
    "    return sentences_pos\n",
    "\n",
    "\n",
    "def posGrams(sentence):\n",
    "    trigram = list(ngrams(sentence, 3))\n",
    "    fourgram = list(ngrams(sentence, 4))\n",
    "    grams = [trigram, fourgram]\n",
    "    return grams\n",
    "\n",
    "\n",
    "def get34grams(sentences_pos):\n",
    "    three_four_grams = []\n",
    "    for sent_pos in sentences_pos:\n",
    "        temp = posGrams(sent_pos)\n",
    "        temp = list(chain(*temp))\n",
    "        three_four_grams.append(temp)\n",
    "    return three_four_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11112\n",
      "['Between one and 10 cases of Congo Fever are reported in South Africa annually, with about 20 to 25 percent of patients dying, according to statistics from the virology institute.', 1]\n",
      "['Saeed said indications were that those tests would be negative too.', 0]\n",
      "['A total of 158 cases of Congo Fever were diagnosed in southern Africa between 1981 and the end of 2000.', 1]\n",
      "['He said it was his opinion that the patient -- a woman -- was suffering from tick bite fever.', 0]\n",
      "['Early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting.', 1]\n",
      "['The two have similar symptoms.', 1]\n",
      "['The disease can be contracted if a person is bitten by a certain tick or if a person comes into contact with the blood of a Congo Fever sufferer.', 1]\n",
      "['The woman was admitted to the hospital on Saturday after complaining of severe joint pains.', 0]\n",
      "['She also had a skin rash and was vomiting.', 1]\n",
      "['The patient told hospital authorities she became sick after being bitten by a tick about four months ago.', 1]\n"
     ]
    }
   ],
   "source": [
    "dataset=joblib.load('dataset_with_labels.sav')\n",
    "print(len(dataset))\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11112\n",
      "8889   2223   8889   2223\n"
     ]
    }
   ],
   "source": [
    "X=np.array(dataset)[:,0] # sentences\n",
    "Y=np.array(dataset)[:,1] # labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=20)\n",
    "\n",
    "print(len(X_train),' ',len(X_test),' ',len(y_train),' ',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3967   4922\n"
     ]
    }
   ],
   "source": [
    "train_facts=[]\n",
    "train_nonfacts=[]\n",
    "for i in range(len(X_train)):\n",
    "    temp=[]\n",
    "    if(y_train[i]=='1'):\n",
    "        temp.append(X_train[i])\n",
    "        temp.append(1)\n",
    "        train_facts.append(temp)\n",
    "    else:\n",
    "        temp.append(X_train[i])\n",
    "        temp.append(0)\n",
    "        train_nonfacts.append(temp)\n",
    "print(len(train_facts),' ',len(train_nonfacts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting wholeSentences for wholedataset, facts, nonfacts as one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1635553   528863   783790\n"
     ]
    }
   ],
   "source": [
    "sent_as_string=getWholeString(dataset)\n",
    "\n",
    "train_facts_as_string=getWholeString(train_facts)\n",
    "train_nonfacts_as_string=getWholeString(train_nonfacts)\n",
    "\n",
    "print(len(sent_as_string) ,' ',len(train_facts_as_string),' ',len(train_nonfacts_as_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting 'nGrams' for 'facts and nonfacts sentences' and converting to 'set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uni: 86806  bi: 86805 tri: 86804\n",
      "10164   53161   78045 \n",
      "===================\n",
      "uni: 127875  bi: 127874 tri: 127873\n",
      "11310   68124   109043 \n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "train_facts_n_grams = getNGrams(train_facts_as_string)\n",
    "train_facts_unigram, train_facts_bigram, train_facts_trigram = list(set(train_facts_n_grams[0])), list(set(train_facts_n_grams[1])), list(set(train_facts_n_grams[2]))\n",
    "print(len(train_facts_unigram), ' ', len(train_facts_bigram), ' ', len(train_facts_trigram), '\\n===================')\n",
    "\n",
    "train_nonfacts_n_grams = getNGrams(train_nonfacts_as_string)\n",
    "train_nonfacts_unigram, train_nonfacts_bigram, train_nonfacts_trigram = list(set(train_nonfacts_n_grams[0])), list(set(train_nonfacts_n_grams[1])), list(set(train_nonfacts_n_grams[2]))\n",
    "print(len(train_nonfacts_unigram), ' ', len(train_nonfacts_bigram), ' ', len(train_nonfacts_trigram),'\\n===================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141370\n",
      "188477\n"
     ]
    }
   ],
   "source": [
    "train_facts_wholeGrams=list(chain(*train_facts_n_grams))\n",
    "train_facts_wholeGrams_list_set=list(set(train_facts_wholeGrams))\n",
    "print(len(train_facts_wholeGrams_list_set))\n",
    "\n",
    "train_nonfacts_wholeGrams=list(chain(*train_nonfacts_n_grams))\n",
    "train_nonfacts_wholeGrams_list_set=list(set(train_nonfacts_wholeGrams))\n",
    "print(len(train_nonfacts_wholeGrams_list_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams_toVector=['facts_unigram','facts_bigram','facts_trigram','nonfacts_unigram','nonfacts_bigram','nonfacts_trigram']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting pos tags and convert into 'set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309474\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "pos=postagging(sent_as_string)\n",
    "\n",
    "print(len(pos))\n",
    "pos_tag=[]\n",
    "\n",
    "for i in pos:\n",
    "    pos_tag.append(i.pos_)\n",
    "pos_tag_list_set=list(set(pos_tag))\n",
    "print(len(pos_tag_list_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting 'named entities' for 'facts and nonfacts sentences' and convert into 'set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15024 --> 17\n",
      "['LOC', 'DATE', 'LANGUAGE', 'PERCENT', 'MONEY', 'GPE', 'QUANTITY', 'ORG', 'ORDINAL', 'PERSON', 'PRODUCT', 'TIME', 'CARDINAL', 'NORP', 'LAW', 'EVENT', 'FAC']\n"
     ]
    }
   ],
   "source": [
    "named_entity=[]\n",
    "for i in pos.ents:\n",
    "    named_entity.append(i.label_)\n",
    "named_entity_list_set=list(set(named_entity))\n",
    "print(len(named_entity),'-->', len(named_entity_list_set))\n",
    "print(named_entity_list_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting 'POS Pattern' and getting '3,4-grams' for each sentence and convert into 'set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_facts=np.array(train_facts)[:,0]\n",
    "X_train_nonfacts=np.array(train_nonfacts)[:,0]\n",
    "# print(X_train_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeTrainFacts=copy.deepcopy(X_train_facts)\n",
    "wholeTrainFacts=getBasicNorm(wholeTrainFacts)\n",
    "\n",
    "wholeNonFacts=copy.deepcopy(X_train_nonfacts)\n",
    "wholeNonFacts=getBasicNorm(wholeNonFacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_posPattern=getPosPattern(wholeTrainFacts)\n",
    "\n",
    "nonfacts_posPattern=getPosPattern(wholeNonFacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'NUM', 'PUNCT', 'SPACE', 'DET', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'VERB', 'DET', 'NOUN', 'PART', 'VERB', 'DET', 'NOUN', 'NOUN', 'ADP', 'SPACE', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT']\n",
      "3967\n",
      "['DET', 'VERB', 'ADV', 'VERB', 'VERB', 'ADV', 'ADJ', 'ADP', 'DET', 'NUM', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'PUNCT']\n",
      "4922\n"
     ]
    }
   ],
   "source": [
    "for i in facts_posPattern:\n",
    "    print(i)\n",
    "    break\n",
    "print(len(facts_posPattern))\n",
    "# ----------------------------------------------------\n",
    "\n",
    "for i in nonfacts_posPattern:\n",
    "    print(i)\n",
    "    break\n",
    "print(len(nonfacts_posPattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9569 \n",
      "\n",
      "10396\n"
     ]
    }
   ],
   "source": [
    "facts_pos_pattern_3_4=get34grams(facts_posPattern)#*****\n",
    "facts_pos_pattern_3_grams,facts_pos_pattern_4_grams=list(set(facts_pos_pattern_3_4[0])),list(set(facts_pos_pattern_3_4[1]))\n",
    "\n",
    "nonfacts_pos_pattern_3_4=get34grams(nonfacts_posPattern)#*****\n",
    "nonfacts_pos_pattern_3_grams,nonfacts_pos_pattern_4_grams=list(set(nonfacts_pos_pattern_3_4[0])),list(set(nonfacts_pos_pattern_3_4[1]))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "facts_pos_pattern_3_4_list=list(chain(*facts_pos_pattern_3_4))\n",
    "facts_pos_pattern_3_4_list_set=list(set(facts_pos_pattern_3_4_list))\n",
    "print(len(facts_pos_pattern_3_4_list_set),'\\n')\n",
    "\n",
    "nonfacts_pos_pattern_3_4_list=list(chain(*nonfacts_pos_pattern_3_4))\n",
    "nonfacts_pos_pattern_3_4_list_set=list(set(nonfacts_pos_pattern_3_4_list))\n",
    "print(len(nonfacts_pos_pattern_3_4_list_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_toVector=['facts_posPattern_3gram','facts_posPattern_4gram','nonfacts_posPattern_3gram','nonfacts_posPattern_4gram',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores=['_Sentiment_Feature_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case1', 'case2', 'case3', 'case4', 'case5']\n"
     ]
    }
   ],
   "source": [
    "Tcases=[]\n",
    "for i in range(1,6):\n",
    "    cases='case'+str(i)\n",
    "    Tcases.append(cases)\n",
    "print(Tcases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding into 'Vector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grams_toVector: 6\n",
      "pos_tag_list_set: 22\n",
      "named_entity_list_set: 39\n",
      "pos_toVector: 43\n",
      "Tpattern: 48\n",
      "sentiment_scores: 49\n"
     ]
    }
   ],
   "source": [
    "vector=[]\n",
    "\n",
    "# Ngrams features\n",
    "# for i in wholeGrams_list_set:\n",
    "#     vector.append(i)\n",
    "# print('wholeGrams_list_set:',len(vector))\n",
    "\n",
    "for i in grams_toVector:\n",
    "    vector.append(i)\n",
    "print('grams_toVector:',len(vector))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# pos Tagging features\n",
    "for i in pos_tag_list_set:\n",
    "    vector.append(i)\n",
    "print('pos_tag_list_set:',len(vector))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Named Entities features\n",
    "for i in named_entity_list_set:\n",
    "    vector.append(i)\n",
    "print('named_entity_list_set:',len(vector))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# posPattern features\n",
    "# for i in pos_pattern_3_4_list_set:\n",
    "#     vector.append(i)\n",
    "# print('pos_pattern_3_4_list_set:',len(vector))\n",
    "\n",
    "for i in pos_toVector:\n",
    "    vector.append(i)\n",
    "print('pos_toVector:',len(vector))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Tpattern cases features\n",
    "for i in Tcases:\n",
    "    vector.append(i)\n",
    "print('Tpattern:',len(vector))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Sentiment Scores features\n",
    "for i in sentiment_scores:\n",
    "    vector.append(i)\n",
    "print('sentiment_scores:',len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['facts_unigram', 'facts_bigram', 'facts_trigram', 'nonfacts_unigram', 'nonfacts_bigram', 'nonfacts_trigram', 'DET', 'PROPN', 'SPACE', 'NUM', 'NOUN', 'ADP', 'PART', 'PUNCT', 'PRON', 'SYM', 'CCONJ', 'X', 'VERB', 'ADJ', 'ADV', 'INTJ', 'LOC', 'DATE', 'LANGUAGE', 'PERCENT', 'MONEY', 'GPE', 'QUANTITY', 'ORG', 'ORDINAL', 'PERSON', 'PRODUCT', 'TIME', 'CARDINAL', 'NORP', 'LAW', 'EVENT', 'FAC', 'facts_posPattern_3gram', 'facts_posPattern_4gram', 'nonfacts_posPattern_3gram', 'nonfacts_posPattern_4gram', 'case1', 'case2', 'case3', 'case4', 'case5', '_Sentiment_Feature_']\n"
     ]
    }
   ],
   "source": [
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving(dumping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sav_1/X_reduced.sav']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_file='sav_1/X_reduced.sav'#only sentences->not labels\n",
    "train_facts_file='sav_1/train_facts_reduced.sav'\n",
    "train_nonfacts_file='sav_1/train_nonfacts_reduced.sav'\n",
    "\n",
    "joblib.dump(train_facts,train_facts_file)\n",
    "joblib.dump(train_nonfacts,train_nonfacts_file)\n",
    "joblib.dump(X,X_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sav_1/train_nonfacts_as_string_reduced.sav']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_as_string_file='sav_1/sent_as_string_reduced.sav'\n",
    "train_facts_as_string_file='sav_1/train_facts_as_string_reduced.sav'\n",
    "train_nonfacts_as_string_file='sav_1/train_nonfacts_as_string_reduced.sav'\n",
    "\n",
    "joblib.dump(sent_as_string,sent_as_string_file)\n",
    "joblib.dump(train_facts_as_string,train_facts_as_string_file)\n",
    "joblib.dump(train_nonfacts_as_string,train_nonfacts_as_string_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sav_1/train_nonfacts_wholeGrams_list_set_reduced.sav']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_facts_unigram_file='sav_1/train_facts_unigram_reduced.sav'\n",
    "train_facts_bigram_file='sav_1/train_facts_bigram_reduced.sav'\n",
    "train_facts_trigram_file='sav_1/train_facts_trigram_reduced.sav'\n",
    "joblib.dump(train_facts_unigram,train_facts_unigram_file)\n",
    "joblib.dump(train_facts_bigram,train_facts_bigram_file)\n",
    "joblib.dump(train_facts_trigram,train_facts_trigram_file)\n",
    "\n",
    "train_nonfacts_unigram_file='sav_1/train_nonfacts_unigram_reduced.sav'\n",
    "train_nonfacts_bigram_file='sav_1/train_nonfacts_bigram_reduced.sav'\n",
    "train_nonfacts_trigram_file='sav_1/train_nonfacts_trigram_reduced.sav'\n",
    "joblib.dump(train_nonfacts_unigram,train_nonfacts_unigram_file)\n",
    "joblib.dump(train_nonfacts_bigram,train_nonfacts_bigram_file)\n",
    "joblib.dump(train_nonfacts_trigram,train_nonfacts_trigram_file)\n",
    "\n",
    "train_facts_wholeGrams_list_set_file='sav_1/train_facts_wholeGrams_list_set_reduced.sav'\n",
    "train_nonfacts_wholeGrams_list_set_file='sav_1/train_nonfacts_wholeGrams_list_set_reduced.sav'\n",
    "joblib.dump(train_facts_wholeGrams_list_set,train_facts_wholeGrams_list_set_file)\n",
    "joblib.dump(train_nonfacts_wholeGrams_list_set,train_nonfacts_wholeGrams_list_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sav_1/pos_tag_list_set_reduced.sav']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_list_set_file='sav_1/pos_tag_list_set_reduced.sav'\n",
    "joblib.dump(pos_tag_list_set,pos_tag_list_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sav_1/named_entity_list_set_reduced.sav']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entity_list_set_file='sav_1/named_entity_list_set_reduced.sav'\n",
    "joblib.dump(named_entity_list_set,named_entity_list_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sav_1/nonfacts_pos_pattern_3_4_list_set_reduced.sav']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts_pos_pattern_3_grams_file='sav_1/facts_pos_pattern_3_grams_reduced.sav'\n",
    "facts_pos_pattern_4_grams_file='sav_1/facts_pos_pattern_4_grams_reduced.sav'\n",
    "nonfacts_pos_pattern_3_grams_file='sav_1/nonfacts_pos_pattern_3_grams_reduced.sav'\n",
    "nonfacts_pos_pattern_4_grams_file='sav_1/nonfacts_pos_pattern_4_grams_reduced.sav'\n",
    "\n",
    "joblib.dump(facts_pos_pattern_3_grams,facts_pos_pattern_3_grams_file)\n",
    "joblib.dump(facts_pos_pattern_4_grams,facts_pos_pattern_4_grams_file)\n",
    "joblib.dump(nonfacts_pos_pattern_3_grams,nonfacts_pos_pattern_3_grams_file)\n",
    "joblib.dump(nonfacts_pos_pattern_4_grams,nonfacts_pos_pattern_4_grams_file)\n",
    "\n",
    "facts_pos_pattern_3_4_list_set_file='sav_1/facts_pos_pattern_3_4_list_set_reduced.sav'\n",
    "nonfacts_pos_pattern_3_4_list_set_file='sav_1/nonfacts_pos_pattern_3_4_list_set_reduced.sav'\n",
    "\n",
    "joblib.dump(facts_pos_pattern_3_4_list_set,facts_pos_pattern_3_4_list_set_file)\n",
    "joblib.dump(nonfacts_pos_pattern_3_4_list_set,nonfacts_pos_pattern_3_4_list_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sav_1/vector_dimensions_reduced.sav']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_file='sav_1/vector_dimensions_reduced.sav'\n",
    "joblib.dump(vector,vector_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_load=joblib.load('sav_1/vector_dimensions_reduced.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
