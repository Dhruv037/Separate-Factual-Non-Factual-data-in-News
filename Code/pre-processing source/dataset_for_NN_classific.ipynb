{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import contractions\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from statistics import mean,stdev\n",
    "from num2words import num2words\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "stop_words = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer('[A-Za-z0-9]*[.]?\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_labels = load('dataset_with_labels.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11112"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('en_wiki_word2vec_300.txt', binary=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "w2v_vocab = set(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_segment(text):\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
    "                        for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while 0 < i:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words, probs[-1]\n",
    "\n",
    "def word_prob(word): \n",
    "    return dictionary[word] / total\n",
    "\n",
    "def words(text): \n",
    "    return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "dictionary = Counter(w2v_vocab)\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num2word_tokens(token):\n",
    "    tokens = []\n",
    "    if token.isdecimal():\n",
    "        token = int(token)\n",
    "    else:\n",
    "        token = float(token)\n",
    "    t = num2words(token)\n",
    "    t = tokenizer.tokenize(t)\n",
    "    for tok in t:\n",
    "        if tok not in stop_words:\n",
    "            tokens.append(tok)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalids1, invalids2 = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_alphab_word(tok, w2v_vocab):\n",
    "    if tok == 'a':\n",
    "        return ['an']\n",
    "    elif tok == 'i':\n",
    "        return ['you']\n",
    "    spell_checked = spell.correction(tok).lower()\n",
    "    tokens = tokenizer.tokenize(spell_checked)\n",
    "    if len(tokens) >1 or tokens[0] in w2v_vocab:\n",
    "#         print('spelling', tokens)\n",
    "        return tokens\n",
    "    token = lemmatizer.lemmatize(tokens[0])\n",
    "    if token in w2v_vocab:\n",
    "#         print(tok, 'lemmat', token)\n",
    "        return [token]\n",
    "    token = stemmer.stem(token)\n",
    "    if token in w2v_vocab:\n",
    "#         print(tok, 'stemming', token)\n",
    "        return [token]\n",
    "    tokens = viterbi_segment(token)[0]\n",
    "    if all(t in w2v_vocab for t in tokens):\n",
    "#         print(tokens)\n",
    "#         invalids1.append(tokens)\n",
    "        return tokens\n",
    "#     invalids2.append(tok)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_num_word(token):\n",
    "    token2 = None\n",
    "    if token[-1] in ['s','m','k']:\n",
    "        token = token[:-1]\n",
    "    elif token[-2:] in ['th','nd','st','rd']:\n",
    "        token = token[:-2]\n",
    "    elif token[-2:] in ['am','pm','bn','km','ft','mm','mg']:\n",
    "        token2 = token[-2:]\n",
    "        token = token[:-2]\n",
    "    else:\n",
    "#         print('dropped', token)\n",
    "        return []\n",
    "    \n",
    "    if all(c in \"0123456789.\" for c in token) and token.count('.') <= 1:\n",
    "        t = get_num2word_tokens(token)\n",
    "        if token2 is not None:\n",
    "            t.append(token2)\n",
    "        return t\n",
    "    else:\n",
    "#         print('dropped', token)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(doc, w2v_vocab):\n",
    "    doc = contractions.fix(doc)\n",
    "    if ',' in doc:\n",
    "        doc = doc.replace(\",\", \"\")\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if len(token) <= 1 and token not in ['a', 'i']:\n",
    "            continue\n",
    "        if all(c in \"0123456789.\" for c in token) and token.count('.') <= 1:\n",
    "            t = get_num2word_tokens(token)\n",
    "            normalized_tokens += t\n",
    "            continue\n",
    "        elif any(c in \"0123456789\" for c in token):\n",
    "            if token not in w2v_vocab:\n",
    "                t = validate_num_word(token)\n",
    "                normalized_tokens += t\n",
    "                continue\n",
    "        else:\n",
    "            token = token.replace(\".\", \"\")\n",
    "            if token not in w2v_vocab:\n",
    "                t = validate_alphab_word(token, w2v_vocab)\n",
    "                normalized_tokens += t\n",
    "                continue\n",
    "        normalized_tokens.append(token)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['desert', 'if']\n",
      "['desert', 'if']\n",
      "spelling ['announced']\n",
      "spelling ['merest']\n",
      "spelling ['debate']\n",
      "spelling ['debate']\n",
      "spelling ['debate']\n",
      "spelling ['debate']\n",
      "spelling ['debate']\n",
      "['abdo', 'rra', 'zaq']\n",
      "['deh', 'dadi']\n",
      "['abdo', 'rra', 'zaq']\n",
      "spelling ['idea']\n",
      "['zvati', 'chada']\n",
      "spelling ['peace']\n",
      "parliamentarians lemmat parliamentarian\n",
      "['anti', 'globalist']\n",
      "['anti', 'globalist']\n",
      "['anti', 'globalist']\n",
      "['anti', 'globalist']\n",
      "spelling ['ramallah']\n",
      "spelling ['kozloduy']\n",
      "spelling ['kozloduy']\n",
      "backpedalled stemming backpedal\n",
      "['would', 'evelop']\n",
      "spelling ['banzai']\n",
      "spelling ['bakufu']\n",
      "spelling ['antithetical']\n",
      "['krum', 'hanzl']\n",
      "spelling ['ayrton']\n",
      "['abdo', 'rrash', 'id']\n",
      "['mawa', 'panga']\n",
      "['mwana', 'nanga']\n",
      "['woul', 'dialogu']\n",
      "['tbe', 'ish', 'at']\n",
      "['tbe', 'ish', 'at']\n",
      "spelling ['indians']\n",
      "spelling ['indians']\n",
      "spelling ['indians']\n",
      "['qalqili', 'yah']\n",
      "['hydrometeor', 'olog']\n",
      "['kuz', 'vart']\n",
      "['uig', 'hh', 'ur']\n",
      "spelling ['banzai']\n",
      "['woul', 'dialogu']\n",
      "['woul', 'dialogu']\n",
      "['environmental', 'ist']\n",
      "spelling ['julian']\n",
      "spelling ['julian']\n",
      "spelling ['purvis']\n",
      "shortsightedness stemming shortsighted\n",
      "['irre', 'spons']\n",
      "['condole', 'eza']\n",
      "enthusiastically stemming enthusiast\n",
      "['giav', 'arini']\n",
      "['giav', 'arini']\n",
      "['giav', 'arini']\n",
      "['giav', 'arini']\n",
      "['giav', 'arini']\n",
      "['under', 'develop']\n",
      "spelling ['rising']\n",
      "['pieter', 'maritzburg']\n",
      "['pieter', 'maritzburg']\n",
      "['pieter', 'maritzburg']\n",
      "['pieter', 'maritzburg']\n",
      "spelling ['wallen']\n",
      "spelling ['wallen']\n",
      "['stoud', 'mann']\n",
      "['baiko', 'nor']\n",
      "spelling ['pique']\n",
      "spelling ['cliff']\n",
      "spelling ['pique']\n",
      "spelling ['cliff']\n",
      "['ros', 'aviaco', 'smo']\n",
      "spelling ['cliff']\n",
      "spelling ['soyuz']\n",
      "['haig', 'ner']\n",
      "['sandu', 'qah']\n",
      "spelling ['nasa']\n",
      "spelling ['vicar']\n",
      "['unco', 'nstitut']\n",
      "spelling ['rusty']\n",
      "['yucel', 'en']\n",
      "['taes', 'upk', 'yok']\n",
      "['yong', 'doja']\n",
      "['live', 'weight']\n",
      "spelling ['students']\n",
      "['kursa', 'nti']\n",
      "spelling ['chin']\n",
      "['intero', 'blast']\n",
      "spelling ['page']\n",
      "occuring stemming occur\n",
      "spelling ['centers']\n",
      "hryvnyas stemming hryvnya\n",
      "spelling ['enterprises']\n",
      "spelling ['martyrs']\n",
      "['hatib', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['alemdar', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['alemdar', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['would', 'ivan']\n",
      "['hatib', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['hatib', 'oglu']\n",
      "['telecomm', 'un']\n",
      "['kurdt', 'el']\n",
      "['telecomm', 'un']\n",
      "['doro', 'dja', 'tun']\n",
      "['doro', 'dja', 'tun']\n",
      "['bregan', 'con']\n",
      "spelling ['miguel']\n",
      "enthusiastically stemming enthusiast\n",
      "occured stemming occur\n",
      "['opuk', 'oma']\n",
      "['nyk', 'op']\n",
      "spelling ['karaman']\n",
      "['avond', 'roodt']\n",
      "spelling ['rabin']\n",
      "spelling ['kiribati']\n",
      "spelling ['kiribati']\n",
      "spelling ['mullah']\n",
      "spelling ['music']\n",
      "spelling ['arm']\n",
      "spelling ['kiribati']\n",
      "spelling ['kiribati']\n",
      "spelling ['kiribati']\n",
      "['macapa', 'nton']\n",
      "spelling ['mercia']\n",
      "['tijo', 'rat']\n",
      "spelling ['forlani']\n",
      "spelling ['wave']\n",
      "spelling ['nasa']\n",
      "spelling ['just']\n",
      "['halil', 'ag']\n",
      "spelling ['declare']\n",
      "spelling ['mangled']\n",
      "['telecomm', 'un']\n",
      "['interc', 'hang']\n",
      "spelling ['guantanamo']\n",
      "spelling ['militant']\n",
      "spelling ['panoply']\n",
      "acknowledgements lemmat acknowledgement\n",
      "misinterpretation stemming misinterpret\n",
      "['rosvo', 'oru', 'zhen', 'iy']\n",
      "['would', 'etai', 'ne']\n",
      "spelling ['presumably']\n",
      "['would', 'irti']\n",
      "spelling ['opportune']\n",
      "spelling ['month']\n",
      "spelling ['guantanamo']\n",
      "spelling ['caruso']\n",
      "spelling ['caruso']\n",
      "spelling ['itemise']\n",
      "spelling ['novel']\n",
      "spelling ['revolution']\n",
      "spelling ['revolution']\n",
      "['constit', 'ut']\n",
      "spelling ['pisan']\n",
      "spelling ['reckon']\n",
      "['transp', 'rovin', 'ci']\n",
      "['ad', 'bolhassan']\n",
      "spelling ['blondes']\n",
      "spelling ['would']\n",
      "['akh', 'nou', 'ch']\n",
      "spelling ['pasha']\n",
      "['myanmar', 'es']\n",
      "['kangan', 'gi']\n",
      "['mura', 'guri']\n",
      "['kangan', 'gi']\n",
      "['kangan', 'gi']\n",
      "['oil', 'bar']\n",
      "spelling ['rosebush']\n",
      "['environmental', 'ist']\n",
      "spelling ['moser']\n",
      "spelling ['meehan']\n",
      "['sherem', 'kulov']\n",
      "['manich', 'ea']\n",
      "['dink', 'ensp', 'iel']\n",
      "spelling ['chinese']\n",
      "['nuclear', 'electrica']\n",
      "['unmon', 'itor']\n",
      "spelling ['heidi']\n",
      "spelling ['heidi']\n",
      "spelling ['heidi']\n",
      "spelling ['defray']\n",
      "spelling ['heidi']\n",
      "spelling ['heidi']\n",
      "['yeni', 'caga']\n",
      "spelling ['syria']\n",
      "spelling ['gce']\n",
      "spelling ['gce']\n",
      "spelling ['gce']\n",
      "spelling ['judicial']\n",
      "spelling ['ofthe']\n",
      "['moz', 'dzy', 'ns', 'ki']\n",
      "['moz', 'dzy', 'ns', 'ki']\n",
      "spelling ['organizers']\n",
      "['lefh', 'oko']\n",
      "['turkmen', 'abad']\n",
      "spelling ['series']\n",
      "spelling ['meshed']\n",
      "spelling ['vyacheslav']\n",
      "['reaks', 'mei']\n",
      "spelling ['kampuchea']\n",
      "spelling ['chivalry']\n",
      "spelling ['chivalry']\n",
      "spelling ['ibo']\n",
      "spelling ['chivalry']\n",
      "['macchia', 'verna']\n",
      "spelling ['democratically']\n",
      "['motsu', 'enyan']\n",
      "spelling ['amoeba']\n",
      "spelling ['amoeba']\n",
      "spelling ['napa']\n",
      "spelling ['ukaea']\n",
      "['kaven', 'del']\n",
      "['kiv', 'riko', 'glu']\n",
      "spelling ['litre']\n",
      "spelling ['considered']\n",
      "['counter', 'viol']\n",
      "spelling ['jiahua']\n",
      "spelling ['jiahua']\n",
      "['motsu', 'enyan']\n",
      "['motsu', 'enyan']\n",
      "['priya', 'tna']\n",
      "['priya', 'tna']\n",
      "['priya', 'tna']\n",
      "['priya', 'tna']\n",
      "['abdur', 'rasyid']\n",
      "['loma', 'hasha']\n",
      "['loma', 'hasha']\n",
      "['mhlume', 'ni']\n",
      "['sits', 'atsa', 'weni']\n",
      "['mafu', 'tse', 'ni']\n",
      "['nkala', 'shan']\n",
      "spelling ['zombis']\n",
      "['map', 'hive', 'ni']\n",
      "spelling ['yau']\n",
      "spelling ['kochen']\n",
      "spelling ['kochen']\n",
      "spelling ['karajan']\n",
      "['ever', 'jo', 'ic']\n",
      "['constit', 'ut']\n",
      "['derk', 'ovs', 'kiy']\n",
      "['enh', 'bayar']\n",
      "['natsa', 'gi', 'yn']\n",
      "['sanz', 'hbe', 'gi', 'yn']\n",
      "spelling ['habash']\n",
      "spelling ['gir']\n",
      "['natsa', 'giy', 'in']\n",
      "['shamay', 'leh']\n",
      "spelling ['polarizing']\n",
      "spelling ['proper']\n",
      "['telecomm', 'un']\n",
      "spelling ['kamm']\n",
      "spelling ['lunatics']\n",
      "spelling ['chianti']\n",
      "spelling ['chianti']\n",
      "spelling ['chianti']\n",
      "spelling ['chianti']\n",
      "['matla', 'shov']\n",
      "['matla', 'shov']\n",
      "['boont', 'ipa']\n",
      "['boont', 'ipa']\n",
      "['simas', 'kul']\n",
      "['unco', 'nstitut']\n",
      "['fede', 'camara']\n",
      "['ubaydi', 'yah']\n",
      "['tsok', 'hatz', 'opou', 'lo']\n",
      "['paskha', 'lid', 'hi']\n",
      "['tsok', 'hatz', 'opou', 'lo']\n",
      "['papayo', 'rgo', 'poulo']\n",
      "spelling ['hezbollah']\n",
      "spelling ['hezbollah']\n",
      "['unco', 'nstitut']\n",
      "['etes', 'elat']\n",
      "spelling ['indian']\n",
      "['telecomm', 'un']\n",
      "spelling ['telescopic']\n",
      "spelling ['indian']\n",
      "spelling ['qaddafi']\n",
      "spelling ['sandinista']\n",
      "spelling ['astronaut']\n",
      "['tikam', 'da']\n",
      "spelling ['amnesty']\n",
      "spelling ['hasty']\n",
      "spelling ['belt']\n",
      "spelling ['varmus']\n",
      "['gadda', 'pora']\n",
      "['haji', 'pora']\n",
      "spelling ['sandra']\n",
      "spelling ['pannella']\n",
      "spelling ['varmus']\n",
      "spelling ['varmus']\n",
      "spelling ['mutual']\n",
      "['security', 'man']\n",
      "spelling ['outside']\n",
      "spelling ['carnal']\n",
      "spelling ['swan']\n",
      "['khane', 'tar']\n",
      "['jumas', 'th']\n",
      "spelling ['laser']\n",
      "spelling ['laser']\n",
      "['gunpo', 'ra']\n",
      "spelling ['naval']\n",
      "spelling ['laser']\n",
      "spelling ['duran']\n",
      "spelling ['opera']\n",
      "spelling ['ivashko']\n",
      "spelling ['ivashko']\n",
      "spelling ['ivashko']\n",
      "spelling ['leonid']\n",
      "spelling ['ivashko']\n",
      "['lamiz', 'idiv', 'ir']\n",
      "['lami', 'vud', 'in']\n",
      "['lamiz', 'idiv', 'ir']\n",
      "['lami', 'vud', 'in']\n",
      "spelling ['zidovudine']\n",
      "['nevir', 'appin']\n",
      "['afa', 'viren']\n",
      "spelling ['adair']\n",
      "spelling ['igora']\n",
      "spelling ['ilie']\n",
      "spelling ['ilie']\n",
      "responsibilities lemmat responsibility\n",
      "spelling ['rocket', 'propelled']\n",
      "spelling ['sharif']\n",
      "['cauter', 'is']\n",
      "intergovernmental stemming intergovernment\n",
      "['tobai', 'wa']\n",
      "spelling ['gupta']\n",
      "['environmental', 'ist']\n",
      "skyjackers stemming skyjack\n",
      "spelling ['athena']\n",
      "responsibilities lemmat responsibility\n",
      "spelling ['yau']\n",
      "spelling ['gce']\n",
      "spelling ['gce']\n",
      "spelling ['gce']\n",
      "['anti', 'dandruff']\n",
      "spelling ['athena']\n",
      "['tobai', 'wa']\n",
      "spelling ['bulawayo']\n",
      "['tobai', 'wa']\n",
      "['tobai', 'wa']\n",
      "['mbuyi', 'so']\n",
      "['saadu', 'din']\n",
      "['environmental', 'ist']\n",
      "['nongreen', 'hous']\n",
      "['turban', 'accord']\n",
      "spelling ['inhumanely']\n",
      "spelling ['commonwealth']\n",
      "spelling ['declared']\n",
      "spelling ['herta']\n",
      "spelling ['stated']\n",
      "spelling ['miguel']\n",
      "['drei', 'zzen']\n",
      "spelling ['miguel']\n",
      "spelling ['counter', 'terrorism']\n",
      "['week', 'cal']\n",
      "spelling ['notably']\n",
      "['environmental', 'ist']\n",
      "['thek', 'yoto']\n",
      "spelling ['curbing']\n",
      "spelling ['yau']\n",
      "spelling ['federal']\n",
      "spelling ['costing']\n",
      "['chapara', 'nd', 'za']\n",
      "['mugu', 'mira']\n",
      "['murcha', 'ba', 'iw']\n",
      "spelling ['chipper']\n",
      "spelling ['growing']\n",
      "['murcha', 'ba', 'iw']\n",
      "spelling ['isolate']\n",
      "spelling ['aidans']\n",
      "spelling ['aidans']\n",
      "['belly', 'ach']\n",
      "['kleb', 'aur']\n",
      "spelling ['footsteps']\n",
      "['above', 'ar']\n",
      "intercontinental stemming intercontinent\n",
      "spelling ['sixteen']\n",
      "spelling ['caring']\n",
      "['tobai', 'wa']\n",
      "['indisc', 'rimin']\n",
      "['chloro', 'fluorocarbon']\n",
      "['nasaw', 'atch', 'com']\n",
      "['globalsecurity', 'org']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spelling ['recently']\n",
      "['telecomm', 'un']\n",
      "['telecomm', 'un']\n",
      "spelling ['markedly']\n",
      "['disen', 'franchis']\n",
      "spelling ['guantanamo']\n",
      "spelling ['behn']\n",
      "spelling ['behn']\n",
      "spelling ['behn']\n",
      "spelling ['behn']\n",
      "spelling ['behn']\n",
      "spelling ['behn']\n",
      "spelling ['behn']\n",
      "spelling ['wobbly']\n",
      "spelling ['maude']\n",
      "['constit', 'ut']\n",
      "['motsu', 'enyan']\n",
      "spelling ['rename']\n",
      "['unself', 'cons', 'ci']\n",
      "spelling ['wouldham']\n",
      "intergovernmental stemming intergovernment\n",
      "spelling ['matcham']\n",
      "spelling ['matcham']\n",
      "['enviro', 'crat']\n",
      "['astronaut', 'when']\n",
      "['lisa', 'ius']\n",
      "spelling ['ricoh']\n",
      "spelling ['astronaut']\n",
      "spelling ['athena']\n",
      "['fue', 'mayor']\n",
      "['fede', 'camara']\n",
      "['fue', 'mayor']\n",
      "spelling ['kazakhstan']\n",
      "parliamentarians lemmat parliamentarian\n",
      "spelling ['schiemann']\n",
      "['alja', 'deeda']\n",
      "spelling ['athan']\n",
      "spelling ['athan']\n",
      "spelling ['countries']\n",
      "spelling ['chavez']\n",
      "spelling ['wanted']\n",
      "['nettles', 'om']\n",
      "['would', 'etai', 'ne']\n",
      "spelling ['emm']\n",
      "spelling ['emm']\n",
      "spelling ['welshman']\n",
      "responsibilities lemmat responsibility\n",
      "['yong', 'doja']\n",
      "responsibilities lemmat responsibility\n",
      "spelling ['precipitation']\n",
      "spelling ['reclaim']\n",
      "spelling ['crowd']\n",
      "spelling ['anna']\n",
      "spelling ['kidson']\n",
      "['incom', 'preh', 'ens']\n",
      "['non', 'belliger']\n",
      "['environmental', 'ist']\n",
      "enthusiastically stemming enthusiast\n",
      "['goma', 'meno']\n",
      "['hagis', 'hiri']\n",
      "spelling ['themselves']\n",
      "['hegemon', 'ist']\n",
      "spelling ['dunums']\n",
      "spelling ['dunums']\n",
      "spelling ['dunums']\n",
      "spelling ['dunums']\n",
      "spelling ['dunums']\n",
      "spelling ['dunums']\n",
      "responsibilities lemmat responsibility\n",
      "intergovernmental stemming intergovernment\n",
      "responsibilities lemmat responsibility\n",
      "['environmental', 'ist']\n",
      "['environmental', 'ist']\n",
      "['environmental', 'ist']\n",
      "['counter', 'product']\n",
      "spelling ['nonexistent']\n",
      "['sumi', 'hiko']\n",
      "spelling ['entered']\n",
      "['danil', 'yan']\n",
      "spelling ['sagdeev']\n",
      "spelling ['sagdeev']\n",
      "['rats', 'ibori', 'nsk', 'iy']\n",
      "['rats', 'ibori', 'nsk', 'iy']\n",
      "['paleo', 'climat', 'olog']\n",
      "spelling ['watch']\n",
      "spelling ['watch']\n",
      "spelling ['watch']\n",
      "['zum', 'rati']\n",
      "spelling ['guantanamo']\n",
      "['would', 'ome', 'st']\n",
      "spelling ['halpern']\n",
      "['kianu', 'shr', 'ad']\n",
      "spelling ['chavez']\n",
      "spelling ['wanted']\n",
      "spelling ['cash']\n",
      "spelling ['iraq']\n",
      "spelling ['iraq']\n",
      "spelling ['iraq']\n",
      "spelling ['cash']\n",
      "spelling ['guangming']\n"
     ]
    }
   ],
   "source": [
    "total_tok = []\n",
    "for x in dataset_with_labels:\n",
    "    tokens = normalize(x[0],w2v_vocab)\n",
    "    total_tok += tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['desert', 'if'],\n",
       " ['desert', 'if'],\n",
       " ['abdo', 'rra', 'zaq'],\n",
       " ['deh', 'dadi'],\n",
       " ['abdo', 'rra', 'zaq'],\n",
       " ['zvati', 'chada'],\n",
       " ['anti', 'globalist'],\n",
       " ['anti', 'globalist'],\n",
       " ['anti', 'globalist'],\n",
       " ['anti', 'globalist'],\n",
       " ['would', 'evelop'],\n",
       " ['krum', 'hanzl'],\n",
       " ['abdo', 'rrash', 'id'],\n",
       " ['mawa', 'panga'],\n",
       " ['mwana', 'nanga'],\n",
       " ['woul', 'dialogu'],\n",
       " ['tbe', 'ish', 'at'],\n",
       " ['tbe', 'ish', 'at'],\n",
       " ['qalqili', 'yah'],\n",
       " ['hydrometeor', 'olog'],\n",
       " ['kuz', 'vart'],\n",
       " ['uig', 'hh', 'ur'],\n",
       " ['woul', 'dialogu'],\n",
       " ['woul', 'dialogu'],\n",
       " ['environmental', 'ist'],\n",
       " ['irre', 'spons'],\n",
       " ['condole', 'eza'],\n",
       " ['giav', 'arini'],\n",
       " ['giav', 'arini'],\n",
       " ['giav', 'arini'],\n",
       " ['giav', 'arini'],\n",
       " ['giav', 'arini'],\n",
       " ['under', 'develop'],\n",
       " ['pieter', 'maritzburg'],\n",
       " ['pieter', 'maritzburg'],\n",
       " ['pieter', 'maritzburg'],\n",
       " ['pieter', 'maritzburg'],\n",
       " ['stoud', 'mann'],\n",
       " ['baiko', 'nor'],\n",
       " ['ros', 'aviaco', 'smo'],\n",
       " ['haig', 'ner'],\n",
       " ['sandu', 'qah'],\n",
       " ['unco', 'nstitut'],\n",
       " ['yucel', 'en'],\n",
       " ['taes', 'upk', 'yok'],\n",
       " ['yong', 'doja'],\n",
       " ['live', 'weight'],\n",
       " ['kursa', 'nti'],\n",
       " ['intero', 'blast'],\n",
       " ['hatib', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['alemdar', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['alemdar', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['would', 'ivan'],\n",
       " ['hatib', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['hatib', 'oglu'],\n",
       " ['telecomm', 'un'],\n",
       " ['kurdt', 'el'],\n",
       " ['telecomm', 'un'],\n",
       " ['doro', 'dja', 'tun'],\n",
       " ['doro', 'dja', 'tun'],\n",
       " ['bregan', 'con'],\n",
       " ['opuk', 'oma'],\n",
       " ['nyk', 'op'],\n",
       " ['avond', 'roodt'],\n",
       " ['macapa', 'nton'],\n",
       " ['tijo', 'rat'],\n",
       " ['halil', 'ag'],\n",
       " ['telecomm', 'un'],\n",
       " ['interc', 'hang'],\n",
       " ['rosvo', 'oru', 'zhen', 'iy'],\n",
       " ['would', 'etai', 'ne'],\n",
       " ['would', 'irti'],\n",
       " ['constit', 'ut'],\n",
       " ['transp', 'rovin', 'ci'],\n",
       " ['ad', 'bolhassan'],\n",
       " ['akh', 'nou', 'ch'],\n",
       " ['myanmar', 'es'],\n",
       " ['kangan', 'gi'],\n",
       " ['mura', 'guri'],\n",
       " ['kangan', 'gi'],\n",
       " ['kangan', 'gi'],\n",
       " ['oil', 'bar'],\n",
       " ['environmental', 'ist'],\n",
       " ['sherem', 'kulov'],\n",
       " ['manich', 'ea'],\n",
       " ['dink', 'ensp', 'iel'],\n",
       " ['nuclear', 'electrica'],\n",
       " ['unmon', 'itor'],\n",
       " ['yeni', 'caga'],\n",
       " ['moz', 'dzy', 'ns', 'ki'],\n",
       " ['moz', 'dzy', 'ns', 'ki'],\n",
       " ['lefh', 'oko'],\n",
       " ['turkmen', 'abad'],\n",
       " ['reaks', 'mei'],\n",
       " ['macchia', 'verna'],\n",
       " ['motsu', 'enyan'],\n",
       " ['kaven', 'del'],\n",
       " ['kiv', 'riko', 'glu'],\n",
       " ['counter', 'viol'],\n",
       " ['motsu', 'enyan'],\n",
       " ['motsu', 'enyan'],\n",
       " ['priya', 'tna'],\n",
       " ['priya', 'tna'],\n",
       " ['priya', 'tna'],\n",
       " ['priya', 'tna'],\n",
       " ['abdur', 'rasyid'],\n",
       " ['loma', 'hasha'],\n",
       " ['loma', 'hasha'],\n",
       " ['mhlume', 'ni'],\n",
       " ['sits', 'atsa', 'weni'],\n",
       " ['mafu', 'tse', 'ni'],\n",
       " ['nkala', 'shan'],\n",
       " ['map', 'hive', 'ni'],\n",
       " ['ever', 'jo', 'ic'],\n",
       " ['constit', 'ut'],\n",
       " ['derk', 'ovs', 'kiy'],\n",
       " ['enh', 'bayar'],\n",
       " ['natsa', 'gi', 'yn'],\n",
       " ['sanz', 'hbe', 'gi', 'yn'],\n",
       " ['natsa', 'giy', 'in'],\n",
       " ['shamay', 'leh'],\n",
       " ['telecomm', 'un'],\n",
       " ['matla', 'shov'],\n",
       " ['matla', 'shov'],\n",
       " ['boont', 'ipa'],\n",
       " ['boont', 'ipa'],\n",
       " ['simas', 'kul'],\n",
       " ['unco', 'nstitut'],\n",
       " ['fede', 'camara'],\n",
       " ['ubaydi', 'yah'],\n",
       " ['tsok', 'hatz', 'opou', 'lo'],\n",
       " ['paskha', 'lid', 'hi'],\n",
       " ['tsok', 'hatz', 'opou', 'lo'],\n",
       " ['papayo', 'rgo', 'poulo'],\n",
       " ['unco', 'nstitut'],\n",
       " ['etes', 'elat'],\n",
       " ['telecomm', 'un'],\n",
       " ['tikam', 'da'],\n",
       " ['gadda', 'pora'],\n",
       " ['haji', 'pora'],\n",
       " ['security', 'man'],\n",
       " ['khane', 'tar'],\n",
       " ['jumas', 'th'],\n",
       " ['gunpo', 'ra'],\n",
       " ['lamiz', 'idiv', 'ir'],\n",
       " ['lami', 'vud', 'in'],\n",
       " ['lamiz', 'idiv', 'ir'],\n",
       " ['lami', 'vud', 'in'],\n",
       " ['nevir', 'appin'],\n",
       " ['afa', 'viren'],\n",
       " ['cauter', 'is'],\n",
       " ['tobai', 'wa'],\n",
       " ['environmental', 'ist'],\n",
       " ['anti', 'dandruff'],\n",
       " ['tobai', 'wa'],\n",
       " ['tobai', 'wa'],\n",
       " ['tobai', 'wa'],\n",
       " ['mbuyi', 'so'],\n",
       " ['saadu', 'din'],\n",
       " ['environmental', 'ist'],\n",
       " ['nongreen', 'hous'],\n",
       " ['turban', 'accord'],\n",
       " ['drei', 'zzen'],\n",
       " ['week', 'cal'],\n",
       " ['environmental', 'ist'],\n",
       " ['thek', 'yoto'],\n",
       " ['chapara', 'nd', 'za'],\n",
       " ['mugu', 'mira'],\n",
       " ['murcha', 'ba', 'iw'],\n",
       " ['murcha', 'ba', 'iw'],\n",
       " ['belly', 'ach'],\n",
       " ['kleb', 'aur'],\n",
       " ['above', 'ar'],\n",
       " ['tobai', 'wa'],\n",
       " ['indisc', 'rimin'],\n",
       " ['chloro', 'fluorocarbon'],\n",
       " ['nasaw', 'atch', 'com'],\n",
       " ['globalsecurity', 'org'],\n",
       " ['telecomm', 'un'],\n",
       " ['telecomm', 'un'],\n",
       " ['disen', 'franchis'],\n",
       " ['constit', 'ut'],\n",
       " ['motsu', 'enyan'],\n",
       " ['unself', 'cons', 'ci'],\n",
       " ['enviro', 'crat'],\n",
       " ['astronaut', 'when'],\n",
       " ['lisa', 'ius'],\n",
       " ['fue', 'mayor'],\n",
       " ['fede', 'camara'],\n",
       " ['fue', 'mayor'],\n",
       " ['alja', 'deeda'],\n",
       " ['nettles', 'om'],\n",
       " ['would', 'etai', 'ne'],\n",
       " ['yong', 'doja'],\n",
       " ['incom', 'preh', 'ens'],\n",
       " ['non', 'belliger'],\n",
       " ['environmental', 'ist'],\n",
       " ['goma', 'meno'],\n",
       " ['hagis', 'hiri'],\n",
       " ['hegemon', 'ist'],\n",
       " ['environmental', 'ist'],\n",
       " ['environmental', 'ist'],\n",
       " ['environmental', 'ist'],\n",
       " ['counter', 'product'],\n",
       " ['sumi', 'hiko'],\n",
       " ['danil', 'yan'],\n",
       " ['rats', 'ibori', 'nsk', 'iy'],\n",
       " ['rats', 'ibori', 'nsk', 'iy'],\n",
       " ['paleo', 'climat', 'olog'],\n",
       " ['zum', 'rati'],\n",
       " ['would', 'ome', 'st'],\n",
       " ['kianu', 'shr', 'ad']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalids1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-7dce2cd6fbf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalids1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minvalids1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalids1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalids1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "print(len(invalids1))\n",
    "invalids1 = set(invalids1)\n",
    "len(invalids1) # 147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalids2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a', 'f', 'i', 'w'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(invalids2))\n",
    "invalids2 = set(invalids2)\n",
    "len(invalids2)\n",
    "invalids2 #5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16219"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(total_tok))\n",
    "total_tok = set(total_tok)\n",
    "len(total_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found = total_tok - w2v_vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_spelled = set()\n",
    "# for word in not_found:\n",
    "#     c = spell.correction(word)\n",
    "#     print(word,'  -  ', c)\n",
    "#     new_spelled.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_dataset_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x in dataset_with_labels:\n",
    "    i += 1\n",
    "    vectors = []\n",
    "    tokens = normalize(x[0],w2v_vocab)\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vec = w2v_model.wv[token]\n",
    "        except:\n",
    "            print(i, token)\n",
    "            continue\n",
    "        vectors.append(vec)\n",
    "    word_embeddings_dataset_1.append([tokens, vectors, x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_embeddings_dataset_1.sav']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(word_embeddings_dataset_1, 'word_embeddings_dataset_1.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_wise_dataset_with_lbs = load('doc_wise_dataset_with_lbs.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_wise_dataset_with_lbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_dataset_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "d = -1\n",
    "j = 0\n",
    "for doc in doc_wise_dataset_with_lbs:\n",
    "    d += 1\n",
    "    i = -1\n",
    "    all_vectors, all_tokens, fact_vectors, fact_tokens = [], [], [], []\n",
    "    for stat in doc:\n",
    "        i += 1\n",
    "        j += 1\n",
    "        tokens = normalize(stat[0],w2v_vocab)\n",
    "        if stat[1] == 0:\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    vec = w2v_model.wv[token]\n",
    "                except:\n",
    "                    print(d, i, token)\n",
    "                    continue\n",
    "                all_vectors.append(vec)\n",
    "            all_tokens += tokens + ['.']\n",
    "            all_vectors.append(np.zeros(300))\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    vec = w2v_model.wv[token]\n",
    "                except:\n",
    "                    print(d, i, token)\n",
    "                    continue\n",
    "                all_vectors.append(vec)\n",
    "                fact_vectors.append(vec)\n",
    "            all_tokens += tokens + ['.']\n",
    "            fact_tokens += tokens + ['.']\n",
    "            all_vectors.append(np.zeros(300))\n",
    "            fact_vectors.append(np.zeros(300))\n",
    "    word_embeddings_dataset_2.append([all_tokens, all_vectors, fact_tokens, fact_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings_dataset_2[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_embeddings_dataset_2.sav']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(word_embeddings_dataset_2, 'word_embeddings_dataset_2.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
